# -*- coding: utf-8 -*-
"""Intro_PySpark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w7U-0XXGCLhOJncXA9ghtieYCodJ153T
"""

#instalação do PySpark
!pip install pyspark

#importação das bibliotecas
import pyspark #biblioteca principal
from pyspark.sql import SparkSession # biblioteca para a criação de uma sessão Spark

# criação de uma sessão spark
spark = SparkSession.builder.appName("Word Count").master('local[*]').getOrCreate()

# criação do SparkContext (conexão entre o Driver e Workers)
sc = spark.sparkContext

rdd = sc.textFile("domcasmurro.txt")

rdd.take(10)

# transformar cada linha (ou seja, cada item da "lista"), em um conjunto de palavras
palavras = rdd.flatMap(lambda x: x.lower().replace(".","").replace(",","").split(" "))
palavras.take(20)

# remoção de vazios
palavras = palavras.filter(lambda x: x!="")
palavras.take(10)

# contagem da qtde de ocorrencias utilizando o countByValue
contagem = palavras.countByValue()
sorted(contagem.items())

# contagem de ocorrencias utilizando chave e valor
palavras_mapeadas = palavras.map(lambda word: (word, 1))
palavras_mapeadas.take(5)

contagem = palavras_mapeadas.reduceByKey(lambda x,y: x+y)
contagem.take(5)

contagem_sorted = contagem.sortByKey()
contagem_sorted.take(10)

# salvando em arquivo
contagem_sorted.coalesce(1).saveAsTextFile("wordcount.txt")

"""##AIRPORTS"""

# carregando o arquivo Airports.csv
rdd_airports = sc.textFile("Airports.csv")
rdd_airports.take(5)

# selecionar as linhas que possuem USA na coluna country
rdd_usa = rdd_airports.filter(lambda x: x.split(",")[4] == "USA")
rdd_usa.take(10)

# qtde de aeroportos que estão nos USA
rdd_usa.count()

# salvar a resposta em um único arquivo
rdd_usa.coalesce(1).saveAsTextFile("airport_usa.txt")

"""Task 1:

 Create a Spark job that loads the file “Airports.csv”, find all airports from Brazil with a latitude greater than 25 degrees. Convert all the outputs to lowercase, sort them according to the latitude in descending order and then save the results in a single file named “Brazil_airports_lat25.txt”.
"""

## Exercicio Semana 12
##Filtrando por 25 graus
rdd_airports = sc.textFile("Airports.csv")
print(rdd_airports.take(3))
print("Fitro por Brazil\n\n")
filtro_Rdd = rdd_airports.filter(lambda x: x.split(",")[4]=='BRAZIL')
print(filtro_Rdd.take(3))
print("Fitro por Latitude\n\n")
filtro_Rdd_maior = filtro_Rdd.filter(lambda x: int(x.split(",")[5])>25)
print(filtro_Rdd_maior.take(3))
print("Map de Conversao lowercase\n\n")
map_lowercase = filtro_Rdd_maior.map(lambda x: x.lower())
print(map_lowercase.take(3))
print("Map de Ordem Crescente: \n\n")
sorted_rdd = map_lowercase.sortBy(lambda x: x.split(",")[5], ascending=False)
print(sorted_rdd.take(10))
sorted_rdd.coalesce(1).saveAsTextFile("Brazil_airports_lat25.txt")

"""Task 2:

 Compute the average latitude of the Brazilian airports
"""

##
rdd_brazil_latitude= map_lowercase.map(lambda x: int(x.split(",")[5]))


soma_latitude = rdd_brazil_latitude.reduce(lambda x,y: x+y)
media_latitude = soma_latitude/rdd_brazil_latitude.count()
print("Media: ",media_latitude)

"""Task 3:

 Find the airports (from any country) with the higher and lower altitude
"""

##remover cabecallho
rdd = rdd_airports.filter(lambda x: x.split(",")[0]!='icao_code')

##
all_altitude = rdd.map(lambda x: int(x.split(",")[13]))
hither_altitude = all_altitude.reduce(lambda x, y: x if x>=y else y)
lower_altitude = all_altitude.reduce(lambda x, y: y if y<=x else x)
print("maior: ",hither_altitude ,"Menor: ",lower_altitude)



airpot_hither_altitude = rdd.filter(lambda x: int(x.split(",")[13])==hither_altitude)

airpot_lower_altitude = rdd.filter(lambda x: int(x.split(",")[13])==lower_altitude)
print("Maior Altura: ",airpot_hither_altitude ,"Menor Altura: ",airpot_lower_altitude)



print("Maior Altura Aeroporto: ",airpot_hither_altitude.collect() ,"Menor Altura Aeroporto: ",airpot_lower_altitude.collect())


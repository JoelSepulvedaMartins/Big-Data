# -*- coding: utf-8 -*-
"""BigDataPySpark

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LmDGVeSI2FA4JZfV7NCwp-A4eqSbDSbs
"""

!pip install pyspark

#importação das bibliotecas
import pyspark #biblioteca principal
from pyspark.sql import SparkSession # biblioteca para a criação de uma sessão Spark

spark = SparkSession.builder.appName("Airpot Paris").master('local[*]').getOrCreate()


sc = spark.sparkContext

rdd = sc.textFile("Airports.csv")
rdd.take(5)

pairRDD = rdd.map(lambda x: (x.split(",")[2], x.split(",")[3]))

#retorna somente as entradas em que a cidade é igual a PARIS
pairRDD_Paris = pairRDD.filter(lambda x: x[1] == "PARIS")
pairRDD_Paris.collect()

##capitalize somente os valores
pairRDD_Paris_capitalized = pairRDD_Paris.mapValues(lambda x: x.capitalize())
pairRDD_Paris_capitalized.collect()

## ordenar os registros com base o nome do aeroporto
pairRDD_Paris_capitalized_sorted = pairRDD_Paris_capitalized.sortByKey(ascending=True)
pairRDD_Paris_capitalized_sorted.collect()

#remover cabeclho
rdd = rdd.filter(lambda x: x.split(",")[0]!="icao_code")

#encontra os egistros coa cidade igual a paris e altitude > 80
pairRDD = rdd.map(lambda x: (x.split(",")[2], (x.split(",")[3],int(x.split(",")[13]))))
pairRDD.take(5)

pairRDD_Paris = pairRDD.filter(lambda x: x[1][0]=="PARIS" and x[1][1]>80)
pairRDD_Paris.collect()

#capitalize somente na cidade
pairRDD_Paris_capitalized = pairRDD_Paris.mapValues(lambda x: (x[0].capitalize(), x[1]))
pairRDD_Paris_capitalized.collect()

#ordena com base a chave
pairRDD_Paris_capitalized_sorted = pairRDD_Paris_capitalized.sortBy(lambda x: x[1][1], ascending=True) ## arrumar
pairRDD_Paris_capitalized_sorted.collect()

##Word count
rdd = sc.textFile("domcasmurro.txt")


rdd_fratten = rdd.flatMap(lambda x: x.split(" "))
#gerando um novo rdd formato (chave,valor -> (word, ocurrence))

pairRDD = rdd_fratten.map(lambda x: (x.lower(), 1))
pairRDD.collect()


#removendo chaves vazias
pairRDD_filtered = pairRDD.filter(lambda x: x[0] != "")

#reduce by key

wordCount = pairRDD_filtered.reduceByKey(lambda x,y: x+y)
wordCount.take(5)

# ordenaçao pela quantidade de ocorrencias
wordCountSorted =wordCount.sortBy(lambda x: x[1], ascending=False)
##wordCountSorted.take(5)

#Atividade3 == quarto 2 == value
rdd = sc.textFile("RealEstate.csv")
rdd = rdd.filter(lambda x: x.split(",")[0]!="MLS")


rdd.take(5)
rdd_fratten = rdd.map(lambda x: x.split(","))
rdd_fratten.take(5)
rddKeyComposite = rdd_fratten.map(lambda x: ((x[4], x[6]),(1)))
rddKeyComposite.take(5)

rdd = sc.textFile("RealEstate.csv")
rdd = rdd.filter(lambda x: x.split(",")[0]!="MLS")
def buid_tuple(s):
  fields = s.split(",")
  return (int(fields[3]), (float(fields[2]),1))

pairRDD =rdd.map(buid_tuple)
pairRDD.take(5)

pairRDDsum = pairRDD.reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1]))
pairRDDsum.take(5)

pairRDD_avarege = pairRDDsum.mapValues(lambda x: x[0]/x[1])
pairRDD_avarege.collect()

pairRDD_avarege_Sorted = pairRDD_avarege.sortByKey(ascending=True)
pairRDD_avarege_Sorted.collect()